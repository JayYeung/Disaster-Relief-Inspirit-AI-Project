{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiM6gYg0nhkY"
      },
      "source": [
        "<font color=\"#de3023\"><h1><b>REMINDER MAKE A COPY OF THIS NOTEBOOK, DO NOT EDIT</b></h1></font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aryCtguFN9e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cccc858f-8aa9-4916-95b7-23e3ba75b5b3"
      },
      "source": [
        "#@title Run this to setup the environment and load the data\n",
        "import re\n",
        "import gdown\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchtext.vocab import GloVe\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "!wget -O ./disaster_data.csv 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Disaster%20Relief/disaster_data.csv'\n",
        "dataset_path = './disaster_data.csv'\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import requests, io, zipfile"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-30 05:28:21--  https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Disaster%20Relief/disaster_data.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.213.128, 173.194.214.128, 173.194.215.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.213.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 236777 (231K) [text/csv]\n",
            "Saving to: ‘./disaster_data.csv’\n",
            "\n",
            "\r./disaster_data.csv   0%[                    ]       0  --.-KB/s               \r./disaster_data.csv 100%[===================>] 231.23K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2021-06-30 05:28:21 (115 MB/s) - ‘./disaster_data.csv’ saved [236777/236777]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix36mXERDGRj",
        "cellView": "form"
      },
      "source": [
        "#@title If the previous cell fails to load data, use this cell\n",
        "import re\n",
        "import gdown\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchtext.vocab import GloVe\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import requests, io, zipfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFnXuHYGRxXN"
      },
      "source": [
        "## Instructor-Led Discussion: Modeling the Meaning of Websites using Word Vectors\n",
        "\n",
        "A shortcoming of our bag-of-words approach is that it only looks at the counts of words in each tweet. What if we had some way of understanding the meaning of words keeping the ordering in mind?\n",
        "\n",
        "The idea of computationally extracting meaning from words is central to word vectors, which have become a cornerstone of modern deep learning on text. Word vectors are a mapping from words to vectors such that words that have similar meaning have similar word vectors.\n",
        "\n",
        "For example, the words \"good\" and \"great\" have similar word vectors, and the words \"good\" and \"planet\" have different word vectors. Thus, word vectors provide us a way to account for the meanings of words with our machine learning models.\n",
        "\n",
        "We will look at GLoVe Embeddings in this section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbW0_pxGhSKM"
      },
      "source": [
        "In this notebook we'll be:\n",
        "*   Exploring Word Similarities\n",
        "*   Visualizing Word Vectors\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfdgTrYyT0Gc"
      },
      "source": [
        "###Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KTWiw50OxEo"
      },
      "source": [
        "# Load the data.\n",
        "disaster_tweets = pd.read_csv('disaster_data.csv',encoding =\"ISO-8859-1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBxY2vr4RkVn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "5f3a27d2-7f51-4269-d02b-81c2b37334f4"
      },
      "source": [
        "disaster_tweets.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>need_or_resource</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ca9e24c8-396d-4502-8b45-18895df5333e_0</td>\n",
              "      <td>Donations of batteries, flashlights, and clean...</td>\n",
              "      <td>Energy</td>\n",
              "      <td>need</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>twitter_resource_tweets_1692</td>\n",
              "      <td>I want hurricane Sandy to cone so I can be stu...</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>625b46e2-0b81-41ea-826e-4535fe9b39b8</td>\n",
              "      <td>Hi, I can help prepare food, serve food, offer...</td>\n",
              "      <td>Food</td>\n",
              "      <td>resource</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>twitter_resource_tweets_1699</td>\n",
              "      <td>I cant believe Sandy.....</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>c3bfea72-d377-445c-b4b8-e8ebca0e7fbb</td>\n",
              "      <td>I have children and adult clothes including ja...</td>\n",
              "      <td>Water</td>\n",
              "      <td>resource</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 tweet_id  ... need_or_resource\n",
              "0  ca9e24c8-396d-4502-8b45-18895df5333e_0  ...             need\n",
              "1            twitter_resource_tweets_1692  ...              NaN\n",
              "2    625b46e2-0b81-41ea-826e-4535fe9b39b8  ...         resource\n",
              "3            twitter_resource_tweets_1699  ...              NaN\n",
              "4    c3bfea72-d377-445c-b4b8-e8ebca0e7fbb  ...         resource\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFarHGU2UXOt"
      },
      "source": [
        "###Extract the tweets and the respective labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiKrF0KFUboE"
      },
      "source": [
        "#Read the tweet data and convert it to lowercase\n",
        "tweets = disaster_tweets['text'].str.lower() \n",
        "tweets = tweets.apply(lambda x: re.sub(r'[^a-zA-Z0-9]+', ' ',x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BluaQntKUdVx"
      },
      "source": [
        "#Extract the labels from the csv\n",
        "tweet_labels = disaster_tweets['category']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO387hafT5H2"
      },
      "source": [
        "###Split the data into train and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMyzuSELT_Fg"
      },
      "source": [
        "#Split the Data into Training and Testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(tweets, tweet_labels, test_size=0.2, random_state=1,stratify = tweet_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQTQZlkBTuKW"
      },
      "source": [
        "###Load the GLoVe Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tob5aXKeRnAe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71b7e403-3752-442e-cd81-cfce617ae54f"
      },
      "source": [
        "VEC_SIZE = 300\n",
        "glove = GloVe(name='6B', dim=VEC_SIZE)\n",
        "\n",
        "# Returns word vector for word if it exists, else return None.\n",
        "def get_word_vector(word):\n",
        "    try:\n",
        "      return glove.vectors[glove.stoi[word.lower()]].numpy()\n",
        "    except KeyError:\n",
        "      return None"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:41, 5.35MB/s]                           \n",
            "100%|█████████▉| 399117/400000 [00:44<00:00, 8543.46it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHQD9FN4Sh6S"
      },
      "source": [
        "We've included a handy helper function which retrieves the word vector for a word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyAdf1dESmcW"
      },
      "source": [
        "##Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qibHFjzeSsO1"
      },
      "source": [
        "Let's retrieve the word vector for \"good\" using the above get_word_vector function (~30 seconds)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kamIu6s4Sg2w"
      },
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n",
        "### END CODE HERE ###\n",
        "\n",
        "print('Shape of good vector:', good_vector.shape)\n",
        "print(good_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWfmLCBZSwUe",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "a9bae0f5-7edb-4938-9e33-fa17c9775194"
      },
      "source": [
        "#@title Instructor solution\n",
        "### YOUR CODE HERE ###\n",
        "good_vector = get_word_vector('good')\n",
        "### END CODE HERE ###\n",
        "\n",
        "print('Shape of good vector:', good_vector.shape)\n",
        "print(good_vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-15f872f33b25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@title Instructor solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m### YOUR CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgood_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_word_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'good'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m### END CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_word_vector' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeB-ADYzS-P4"
      },
      "source": [
        "Well not much to see here–each word vector is a vector of 300 numbers, and it's hard to interpret them from looking at the numbers. Remember that the important property of word vectors is that words with similar meaning have similar word vectors. The magic happens when we compare word vectors.\n",
        "\n",
        "Below, we have set up a demo where we compare the word vectors for two words using a comparison metric known as cosine similarity. Intuitively, cosine similarity measures the extent to which two vectors point in the same direction. You might be familiar with the fact that the cosine similarity between two vectors is the same as the cosine of the angle between the two vectors–ranging between -1 and 1. -1 means that two vectors are facing opposite directions, 0 means that they are perpindicular, and 1 means that they are facing the same direction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTIpLdgsTJJD"
      },
      "source": [
        "##Instructor-Led Discussion: Comparing Word Similarities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZubOH_J9TOLz"
      },
      "source": [
        "Try running the below to compare the vectors for \"good\" and \"great\", and then try other words, like \"planet\". What do you notice that's expected and unexpected? Play around for a couple of minutes then discuss as a class.\n",
        "\n",
        "Note that the demo runs automatically when you change either word1 or word2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To_1Uy8_TCZ1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "ccd0be8d-e7d8-4780-c7e0-830754ca098c"
      },
      "source": [
        "#@title Word Similarity { run: \"auto\", display-mode: \"both\" }\n",
        "\n",
        "def cosine_similarity(vec1, vec2):    \n",
        "  return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "word1 = \"mumbai\" #@param {type:\"string\"}\n",
        "word2 = \"delhi\" #@param {type:\"string\"}\n",
        "\n",
        "print('Word 1:', word1)\n",
        "print('Word 2:', word2)\n",
        "\n",
        "def cosine_similarity_of_words(word1, word2):\n",
        "  vec1 = get_word_vector(word1)\n",
        "  vec2 = get_word_vector(word2)\n",
        "  \n",
        "  if vec1 is None:\n",
        "    print(word1, 'is not a valid word. Try another.')\n",
        "  if vec2 is None:\n",
        "    print(word2, 'is not a valid word. Try another.')\n",
        "  if vec1 is None or vec2 is None:\n",
        "    return None\n",
        "  \n",
        "  return cosine_similarity(vec1, vec2)\n",
        "  \n",
        "\n",
        "print('\\nCosine similarity:', cosine_similarity_of_words(word1, word2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word 1: mumbai\n",
            "Word 2: delhi\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6f6cfa6fc8fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nCosine similarity:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosine_similarity_of_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-6f6cfa6fc8fa>\u001b[0m in \u001b[0;36mcosine_similarity_of_words\u001b[0;34m(word1, word2)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcosine_similarity_of_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mvec1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_word_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mvec2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_word_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_word_vector' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkPgrLPmTXSY"
      },
      "source": [
        "We can see that word embeddings appear to capture the meaning of different words–when two words are similar, the cosine similarity score is higher, and when two words are dissimilar, the cosine similarity score is lower.\n",
        "\n",
        "Word vectors are created by going over a large body of text (the vectors you are using were trained on Wikipedia in part) and noticing which words tend to occur near each-other. If word A tends to co-occur with similar words as word B, then the word vectors for words A and B are mathematically constrained to be similar. If you want to learn more about an algorithm for training word vectors, see this [helpful introduction to word2vec](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa).\n",
        "\n",
        "Given word vectors that represent the meaning of words, what can we do with this? We can add word vectors to our feature vector, but which do we choose? It turns out that a solid approach is just to average the word vectors for all the words in the description. Averaging word vectors produces a natural way to produce vectors for sentences and other collections of words, and this is the approach we will use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjudMdP2TfrG"
      },
      "source": [
        "## Exercise \n",
        "\n",
        "We want to write a function that takes a list of descriptions and turns it into an array containing the average GloVe vector for each description. **Understand the code below and then increment found_words and add vec to X[i].**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnBXudzmTQgH"
      },
      "source": [
        "def glove_transform_data_descriptions(descriptions):\n",
        "    X = np.zeros((len(descriptions), VEC_SIZE))\n",
        "    for i, description in enumerate(descriptions):\n",
        "        found_words = 0.0\n",
        "        description = description.strip()\n",
        "        for word in description.split(): \n",
        "            vec = get_word_vector(word)\n",
        "            if vec is not None:\n",
        "                ### YOUR CODE HERE ###\n",
        "                # Increment found_words and add vec to X[i].\n",
        "\n",
        "                \n",
        "                ### END CODE HERE ###\n",
        "        # We divide the sum by the number of words added, so we have the\n",
        "        # average word vector.\n",
        "        if found_words > 0:\n",
        "            X[i] /= found_words\n",
        "            \n",
        "    return X\n",
        "  \n",
        "glove_train_X = glove_transform_data_descriptions(X_train)\n",
        "glove_train_y = [l for label in y_train]\n",
        "\n",
        "glove_test_X = glove_transform_data_descriptions(X_test)\n",
        "glove_test_y = [l for label in y_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhUZVMP6V6z9"
      },
      "source": [
        "#@title Instructor solution\n",
        "def glove_transform_data_descriptions(data):\n",
        "    X = np.zeros((len(data), VEC_SIZE))\n",
        "    \n",
        "    \n",
        "    for i, tweet in enumerate(data):\n",
        "        found_words = 0.0\n",
        "        tweet = tweet.strip()\n",
        "        for word in tweet.split():\n",
        "           \n",
        "            vec = get_word_vector(word)\n",
        "            if vec is not None:\n",
        "                ### YOUR CODE HERE ###\n",
        "                # Increment found_words and add vec to X[i].\n",
        "                found_words += 1\n",
        "                X[i] += vec\n",
        "                ### END CODE HERE ###\n",
        "        # We divide the sum by the number of words added, so we have the\n",
        "        # average word vector.\n",
        "        if found_words > 0:\n",
        "            X[i] /= found_words\n",
        "            \n",
        "    return X\n",
        "  \n",
        "glove_train_X = glove_transform_data_descriptions(X_train)\n",
        "glove_train_y = [label for label in y_train]\n",
        "print (\"Train_shape\",glove_train_X.shape)\n",
        "glove_test_X = glove_transform_data_descriptions(X_test)\n",
        "glove_test_y = [label for label in y_test]\n",
        "print (\"Test shape\",glove_test_X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrlEEN6pXT1S"
      },
      "source": [
        "## Exercise \n",
        "\n",
        "Then, we can evaluate our approach as we have in the past. As before, fill in the code for fitting and evaluation (~8 minutes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y0z-uQ6XS7z"
      },
      "source": [
        "model = LogisticRegression()\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJiw66laXk2C",
        "cellView": "form"
      },
      "source": [
        "#@title Instructor solution\n",
        "model = LogisticRegression()\n",
        "### YOUR CODE HERE ###\n",
        "model.fit(glove_train_X, glove_train_y)\n",
        "\n",
        "glove_train_y_pred = model.predict(glove_train_X)\n",
        "print('Train accuracy', accuracy_score(glove_train_y, glove_train_y_pred))\n",
        "\n",
        "glove_test_y_pred = model.predict(glove_test_X)\n",
        "print('Val accuracy', accuracy_score(glove_test_y, glove_test_y_pred))\n",
        "\n",
        "print('Confusion matrix:')\n",
        "print(confusion_matrix(glove_test_y, glove_test_y_pred))\n",
        "\n",
        "prf = precision_recall_fscore_support(glove_test_y, glove_test_y_pred)\n",
        "\n",
        "print('Precision:', prf[0][1])\n",
        "print('Recall:', prf[1][1])\n",
        "print('F-Score:', prf[2][1])\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va6CVv0SoEOt"
      },
      "source": [
        "###Exercise(Discussion): Why do you think the accuarcy didnt change much even though we introduced word embeddings as our features?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYccPzP4Ykym"
      },
      "source": [
        "print(classification_report(y_test,glove_test_y_pred, target_names=['Energy', 'Food', 'Medical', 'None', 'Water']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZOKzdGObUKh",
        "cellView": "form"
      },
      "source": [
        "#@title Helper Function-Confusion Matrix\n",
        "'''\n",
        "Plots the confusion Matrix and saves it\n",
        "'''\n",
        "def plot_confusion_matrix(y_true,y_predicted):\n",
        "  cm = metrics.confusion_matrix(y_true, y_predicted)\n",
        "  print (\"Plotting the Confusion Matrix\")\n",
        "  labels = ['Energy', 'Food', 'Medical', 'None', 'Water']\n",
        "  df_cm = pd.DataFrame(cm,index =labels,columns = labels)\n",
        "  fig = plt.figure()\n",
        "  res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')\n",
        "  plt.yticks([0.5,1.5,2.5,3.5,4.5], labels,va='center')\n",
        "  plt.title('Confusion Matrix - TestData')\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZa0F_7qbZt0"
      },
      "source": [
        "plot_confusion_matrix(y_test,glove_test_y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNLOH21heC3G"
      },
      "source": [
        "###Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8zuQKqYeTNE"
      },
      "source": [
        "*Let's see how our classifier did! We will train our classifier on 80% of the dataset and then test it on 20%. This is called a train-test split and is usually done to evaluate models.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1qWUIs_tnee",
        "cellView": "form"
      },
      "source": [
        "#@title Get the list of incorrect tweets\n",
        "pd.set_option('max_colwidth', 500)\n",
        "incorrect_tweets = []\n",
        "incorrect_y_test = []\n",
        "incorrect_y_pred = []\n",
        "for (t,x,y) in zip(X_test,y_test,glove_test_y_pred):\n",
        "  if x != y:\n",
        "    incorrect_tweets.append(t)\n",
        "    incorrect_y_test.append(x)\n",
        "    incorrect_y_pred.append(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxglAdpaeSf4"
      },
      "source": [
        "table=pd.DataFrame([incorrect_tweets,incorrect_y_pred,incorrect_y_test]).transpose()\n",
        "table.columns = ['Tweet', 'Predicted Category', 'True Category']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGJpRZjOeE29"
      },
      "source": [
        "table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoOal-XMkJ2R"
      },
      "source": [
        "###Exercise(Discussion): Can you figure out why some of these tweets were incorrectly classified?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHKXjfBKvDjh"
      },
      "source": [
        "###Visualizing Word Vectors with t-SNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmfV7ujlksjg"
      },
      "source": [
        "We will plot the words using the word embeddings in this section to derive relationships based on the context of the tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6xSUQoDwWwU",
        "cellView": "form"
      },
      "source": [
        "#@title Helper Function to Visualize the Embeddings\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def clean(text):\n",
        "    \"\"\"Remove posting header, split by sentences and words, keep only letters\"\"\"\n",
        "    lines = re.split('[?!.:]\\s', re.sub('^.*Lines: \\d+', '', re.sub('\\n', ' ', text)))\n",
        "    return [re.sub('[^a-zA-Z]', ' ', line).lower().split() for line in lines]\n",
        "\n",
        "sentences = [line for text in tweets for line in clean(text)]\n",
        "\n",
        "#min-count variable helps us eliminate the words which rarely occur! \n",
        "model = Word2Vec(sentences, workers=4, size=100, min_count=30, window=10, sample=1e-3)\n",
        "\n",
        "\n",
        "def tsne_plot(model):\n",
        "    \"Creates and TSNE model and plots it\"\n",
        "    labels = []\n",
        "    tokens = []\n",
        "\n",
        "    for word in model.wv.vocab:\n",
        "        tokens.append(model[word])\n",
        "        labels.append(word)\n",
        "    \n",
        "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
        "    new_values = tsne_model.fit_transform(tokens)\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "    for value in new_values:\n",
        "        x.append(value[0])\n",
        "        y.append(value[1])\n",
        "        \n",
        "    plt.figure(figsize=(16, 16)) \n",
        "    for i in range(len(x)):\n",
        "        plt.scatter(x[i],y[i])\n",
        "        plt.annotate(labels[i],\n",
        "                     xy=(x[i], y[i]),\n",
        "                     xytext=(5, 2),\n",
        "                     textcoords='offset points',\n",
        "                     ha='right',\n",
        "                     va='bottom')\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7FmjhbkZawM"
      },
      "source": [
        "tsne_plot(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViStd1wJnpQO"
      },
      "source": [
        "###Exercise(Discussion): Do you notice that similar words are placed close by?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNkVVIP4n0mz"
      },
      "source": [
        "#Finish!"
      ]
    }
  ]
}